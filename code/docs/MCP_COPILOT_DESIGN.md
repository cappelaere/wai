# MCP Copilot - Detailed Design Document

## System Overview

A web-based AI copilot assistant powered by the Model Context Protocol (MCP), designed to help users analyze and compare scholarship applications with a single-application focus model. The copilot uses specialized MCP servers to access data and perform operations.

## Core Design Principles

1. **Single Application Focus** - Context revolves around one application being analyzed
2. **Multi-MCP Server Architecture** - Each domain has its own MCP server
3. **Stateless API** - Session state stored separately from compute logic
4. **Tool-Driven Agent** - Copilot uses MCP tools to gather data and perform analysis
5. **Extensible Tool Registry** - Easy to add new capabilities without modifying core

## User Interaction Flow

```
User selects Application A
        ↓
User: "What are the key strengths?"
        ↓
Copilot receives query with context:
{
  query: "What are the key strengths?",
  session_id: "session_123",
  current_application: "app_5678"
}
        ↓
Copilot determines it needs:
  - Application data (via ApplicationDataMCPServer)
  - Profile analysis (via AnalysisMCPServer)
        ↓
MCP calls fetch:
  - Application profiles (all 5 types)
  - Application metadata
        ↓
Ollama analyzes with tool results
        ↓
Response: "Based on the analysis, key strengths are..."
        ↓
Context updated with current_application
        ↓
User: "How does this compare to others?"
        ↓
Context provides current_application
        ↓
Copilot performs comparison (same app in focus)
```

## MCP Server Specifications

### 1. ApplicationDataMCPServer

**Purpose**: Access and query application data

**Tools**:
```
get_application(app_id: str) → ApplicationData
  Returns: {
    id: str,
    name: str,
    scholarship: str,
    status: str,
    submission_date: str,
    metadata: {...}
  }

get_application_profiles(app_id: str) → Profiles
  Returns: {
    application_profile: {...},
    personal_profile: {...},
    recommendation_profile: {...},
    academic_profile: {...},
    social_profile: {...}
  }

search_applications(criteria: dict) → List[ApplicationRef]
  Criteria:
    - scholarship: str
    - status: str
    - gpa_min: float
    - gpa_max: float
    - page: int
    - limit: int
  Returns list of matching applications with basic info

get_scholarship_applications(scholarship_name: str) → List[ApplicationRef]
  Returns all applications for a scholarship

get_application_metadata(app_id: str) → Metadata
  Returns structured metadata about application
```

**Data Sources**:
- Reads from: `output/{scholarship}/{app_id}/`
- JSON files: `application_profile.json`, `personal_profile.json`, etc.
- Index/cache of applications for fast lookup

### 2. AnalysisMCPServer

**Purpose**: Perform analytical operations on application data

**Tools**:
```
analyze_application_strengths(app_id: str) → StrengthsAnalysis
  Analyzes application profiles to identify:
    - Academic strengths
    - Personal qualities
    - Leadership indicators
    - Unique achievements
  Returns: {strengths: [...], confidence_scores: {...}}

analyze_application_weaknesses(app_id: str) → WeaknessAnalysis
  Identifies potential concerns:
    - Gaps in experience
    - Lower metrics
    - Areas for improvement
  Returns: {weaknesses: [...], severity: {...}}

compare_applications(app_ids: List[str]) → Comparison
  Compares 2+ applications:
    - Score each application
    - Highlight differences
    - Rank by criteria
  Returns: {comparison_matrix: {...}, rankings: {...}}

calculate_application_score(app_id: str, criteria: List[str]) → Score
  Calculates score based on specified criteria
  Criteria: academic, leadership, impact, uniqueness, etc.
  Returns: {score: float, breakdown: {...}}

identify_patterns(app_ids: List[str], field: str) → Patterns
  Identifies patterns across applications
  Examples: "common_gpa_range", "leadership_themes"
  Returns: {patterns: [...], frequency: {...}}
```

**Data Sources**:
- Uses ApplicationDataMCPServer to fetch raw data
- Performs analysis locally
- Can leverage processor output (profiles already generated by AI)

### 3. ContextMCPServer

**Purpose**: Manage session and conversation context

**Tools**:
```
set_current_application(session_id: str, app_id: str) → bool
  Sets the application being discussed in this session
  Returns success/failure

get_current_application(session_id: str) → ApplicationRef
  Returns the application currently in focus for this session
  Returns: {id: str, name: str, scholarship: str}

save_context(session_id: str, key: str, value: any) → bool
  Save arbitrary context data
  Examples:
    - "filter_criteria": {gpa_min: 3.5}
    - "comparison_baseline": "app_123"
    - "analysis_focus": "academic"

get_context(session_id: str, key: str) → any
  Retrieve saved context data

get_conversation_history(session_id: str, limit: int = 10) → List[Message]
  Returns recent conversation messages
  Returns: [{role: "user"|"assistant", content: str, timestamp: str}, ...]

clear_session(session_id: str) → bool
  Clear all context for a session
```

**Data Storage**:
- Session data: Database or Redis
- Structure:
  ```
  {
    session_id: str,
    user_id: str,
    current_application: str,
    custom_context: {...},
    created_at: datetime,
    last_accessed: datetime
  }
  ```

### 4. ProcessorMCPServer

**Purpose**: Interface with the scholarship processor pipeline

**Tools**:
```
get_processor_status() → ProcessorStatus
  Returns: {
    running: bool,
    current_step: int,
    applications_processed: int,
    estimated_time_remaining: int
  }

get_processor_output_path(app_id: str, step: int) → str
  Returns path to output for application at specific step

get_step_output(app_id: str, step: int) → StepOutput
  Returns output from specific processing step
  Examples: extracted text, generated profiles, reports

explain_processor_step(step: int) → Explanation
  Returns human-readable explanation of what a step does
  Used for user guidance

verify_application_processed(app_id: str) → bool
  Check if application has been fully processed
  Returns: bool indicating if all steps completed
```

**Data Sources**:
- Reads from: `output/{scholarship}/{app_id}/`
- Accesses processor step outputs
- Integration with processor pipeline

## Tool Registry Design

**Purpose**: Central catalog of all MCP tools with metadata

**Structure**:
```python
ToolRegistry {
  tools: {
    "get_application": {
      server: "application_data",
      name: "get_application",
      description: "Retrieve application data by ID",
      input_schema: {...},
      return_schema: {...},
      examples: [...]
    },
    "analyze_application_strengths": {
      server: "analysis",
      name: "analyze_application_strengths",
      ...
    },
    ...
  }
}
```

**Functions**:
- `register(server_name, tool_schema)` - Add tool
- `get_tool(name)` - Get tool schema
- `get_server_tools(server_name)` - Get all tools for server
- `get_all_tools()` - Get all available tools
- `validate_call(tool_name, arguments)` - Validate tool arguments

## Copilot Agent Design

### Intent Classification

**Input**: User query + current context
**Output**: Intent type with confidence

**Intent Types**:
1. **ANALYZE_APPLICATION** - "What are the strengths of this application?"
   - Requires: get_application_profiles, analyze_application_strengths
2. **COMPARE_APPLICATIONS** - "How does this compare to others?"
   - Requires: compare_applications
3. **SEARCH_APPLICATIONS** - "Find applications with high GPA"
   - Requires: search_applications
4. **GET_INFORMATION** - "What is the recommendation score?"
   - Requires: get_application_profiles
5. **PROCESSOR_GUIDANCE** - "How does the processor work?"
   - Requires: explain_processor_step

### Agent Algorithm

```
1. Receive(query, session_id, current_application)

2. Load session context:
   context = load_context(session_id)
   current_app = context.current_application or param.current_application

3. Classify intent:
   intent, confidence = classify_intent(query)

4. If confidence < threshold:
   return "I'm not sure how to help with that. Could you rephrase?"

5. Get required tools for intent:
   required_tools = get_tools_for_intent(intent)

6. Build system prompt:
   prompt = build_prompt(intent, required_tools, current_app)

7. Call Ollama with tools:
   response = ollama.chat(
     messages=[{role: "system", content: prompt}, {role: "user", content: query}],
     tools=required_tools
   )

8. Execute tool calls if present:
   while response.tool_calls:
     for tool_call in response.tool_calls:
       result = mcp_client.call_tool(tool_call.name, tool_call.args)
     response = ollama.chat(..., tool_results)

9. Update context:
   update_context(session_id, {...})

10. Return response
```

### System Prompt Template

```
You are a scholarly analysis assistant helping users evaluate scholarship applications.

Context:
- Current application: {app_name} (ID: {app_id})
- Scholarship: {scholarship_name}
- Mode: Single application focus

Available tools:
{tools_list_with_schemas}

Instructions:
1. Use tools to gather data before analyzing
2. Always reference specific data from profiles
3. Provide actionable insights
4. When comparing, always reference the current application
5. Maintain professional tone
6. Be honest about data limitations
```

## Context Model

**Session Context**:
```
{
  session_id: str,              # Unique session identifier
  user_id: str,                 # User who created session
  current_application: str,     # App ID in focus (nullable)
  current_scholarship: str,     # Scholarship in focus (nullable)
  conversation_history: [...],  # Recent messages
  custom_context: {             # User-defined context
    filter_criteria: {...},
    analysis_focus: str,
    comparison_baseline: str,
    ...
  },
  metadata: {
    created_at: datetime,
    last_accessed: datetime,
    message_count: int
  }
}
```

**Context Lifecycle**:
1. Create session when user starts
2. Update current_application when user selects app
3. Save custom context as conversation progresses
4. Update last_accessed on each interaction
5. Expire session after timeout (default 1 hour)

## Data Access Layer Design

### ApplicationDataService

**Responsibilities**:
- Load application profiles from disk
- Index applications for fast lookup
- Cache frequently accessed data
- Validate application IDs

**Key Methods**:
```
get_application(app_id: str) -> ApplicationData
get_application_profiles(app_id: str) -> ProfilesData
search_applications(scholarship: str, criteria: dict) -> List
list_scholarships() -> List[str]
index_applications() -> Index
```

**Data Storage**:
```
output/
├── {scholarship}/
│   ├── {app_id}/
│   │   ├── application_form_data.json
│   │   ├── application_form_text.txt
│   │   ├── attachments.json
│   │   ├── application_profile.json
│   │   ├── personal_profile.json
│   │   ├── recommendation_profile.json
│   │   ├── academic_profile.json
│   │   └── social_profile.json
```

### SessionService

**Responsibilities**:
- Create and manage sessions
- Store/retrieve context
- Handle session expiration
- Clean up old sessions

**Storage Options**:
- Option 1: SQLite (file-based, simple)
- Option 2: PostgreSQL (if scaling needed)
- Option 3: Redis (if high performance needed)

**Schema**:
```
sessions(
  session_id PRIMARY KEY,
  user_id,
  current_application,
  context JSON,
  created_at,
  last_accessed,
  expires_at
)

conversation_history(
  id PRIMARY KEY,
  session_id FOREIGN KEY,
  role (user|assistant),
  content,
  timestamp
)
```

## API Design

### REST Endpoints

```
POST /api/v1/sessions
  Create new session
  Request: {user_id: str}
  Response: {session_id: str, expires_at: datetime}

GET /api/v1/sessions/{session_id}
  Get session context
  Response: {current_application: str, context: {}, history: [...]}

PUT /api/v1/sessions/{session_id}
  Update session context
  Request: {current_application: str, context: {...}}
  Response: {updated: bool}

POST /api/v1/chat
  Send query to copilot
  Request: {
    session_id: str,
    query: str,
    current_application: str (optional)
  }
  Response: {
    response: str,
    tools_used: [str],
    metadata: {...}
  }

GET /api/v1/tools
  Get available tools
  Response: {tools: [...]}

DELETE /api/v1/sessions/{session_id}
  End session
  Response: {deleted: bool}
```

### WebSocket Endpoint

```
WS /ws/chat/{session_id}
  Real-time chat connection

  Client → Server:
  {
    type: "query",
    content: str,
    current_application: str
  }

  Server → Client:
  {
    type: "thinking",
    content: str
  }
  or
  {
    type: "tool_call",
    tool: str,
    args: {...}
  }
  or
  {
    type: "response",
    content: str,
    tools_used: [str]
  }
```

## Error Handling Design

### Error Types

1. **Input Validation Error**
   - Invalid session ID
   - Invalid application ID
   - Malformed query
   - Action: Return 400 Bad Request

2. **Tool Execution Error**
   - Tool fails to execute
   - Data not found
   - Action: Fallback to general response, inform user of limitation

3. **LLM Error**
   - Ollama unavailable
   - Model not found
   - Action: Return 503 Service Unavailable

4. **Context Error**
   - Session expired
   - Context corrupted
   - Action: Create new session, inform user

### Error Response Format

```json
{
  "error": {
    "code": "INVALID_APPLICATION",
    "message": "Application not found",
    "details": {...},
    "recovery_action": "Please select a valid application"
  }
}
```

## Security Design

### Session Management
- Session tokens are random UUIDs
- Sessions expire after inactivity
- CORS configured for allowed origins
- HTTPS required in production

### Input Validation
- All user inputs validated against schema
- Tool arguments validated before execution
- Query length limits (prevent DoS)
- Rate limiting on API endpoints

### Data Access
- Applications only accessible if in correct scholarship folder
- No cross-user data leakage
- Audit logging of tool calls (optional)

## Performance Design

### Caching Strategy
- Cache application index (refresh on startup)
- Cache tool schemas (immutable)
- Cache session data (expires with session)
- LRU cache for frequent queries

### Response Streaming
- Stream Ollama responses to client (if supported)
- Real-time tool call feedback via WebSocket
- Prevent blocking on long operations

### Database Optimization
- Index on session_id for fast lookup
- Index on application_id for data access
- Batch operations where possible
- Connection pooling

## Testing Design

### Unit Tests
- Tool schema validation
- Intent classification
- Context management
- Error handling

### Integration Tests
- MCP server → tool execution
- Ollama integration
- End-to-end query processing
- Session management

### Load Tests
- Concurrent sessions
- Rapid-fire queries
- Tool call throughput
- Response time under load

## Deployment Design

### Container Strategy
```dockerfile
# Multi-stage build
FROM python:3.13-slim as builder
# Install dependencies

FROM python:3.13-slim
# Copy dependencies
# Copy code
# Run server
```

### Environment Configuration
```
.env.development
.env.staging
.env.production
```

### Health Checks
- `/health` - Basic health check
- `/health/ollama` - Check Ollama connection
- `/health/db` - Check database connection

## Documentation Design

### For Developers
- MCP server implementation guide
- Tool schema specification
- API endpoint documentation
- Agent algorithm explanation

### For Users
- How to use the copilot
- Available capabilities
- Tips for effective queries
- Limitations and constraints

## Scalability Considerations

### Horizontal Scaling
- Stateless API servers (load balanced)
- Shared session storage (DB)
- Shared application data (mounted filesystem)

### Vertical Scaling
- Connection pooling
- Caching optimization
- Database indexing
- Query optimization

### Future Extensions
- Add more MCP servers
- Support multiple models
- Add user preferences
- Add conversation templates
